{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ac400b2",
   "metadata": {},
   "source": [
    "检测是否有MNIST数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a7249a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_set = datasets.MNIST(\"data\",train=True,download=True, transform=transforms.ToTensor(),)\n",
    "test_set = datasets.MNIST(\"data\",train=False,download=True, transform=transforms.ToTensor(),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482f97b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.nn import Conv2d, Linear, ReLU\n",
    "from torch.nn import MaxPool2d\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Dataset:创建数据集的函数；__init__:初始化数据内容和标签\n",
    "# __geyitem:获取数据内容和标签\n",
    "# __len__:获取数据集大小\n",
    "# daataloader:数据加载类，接受来自dataset已经加载好的数据集\n",
    "# torchbision:图形库，包含预训练模型，加载数据的函数、图片变换，裁剪、旋转等\n",
    "# torchtext:处理文本的工具包，将不同类型的额文件转换为datasets\n",
    "\n",
    "# 预处理：将两个步骤整合在一起\n",
    "transform = transforms.Compose({\n",
    "    transforms.ToTensor(),  # 将灰度图片像素值（0~255）转为Tensor（0~1），方便后续处理\n",
    "   })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb0dfa9",
   "metadata": {},
   "source": [
    "检测一下是否能加载数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1acec789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练数据集的长度：60000\n",
      "测试数据集的长度：10000\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "# 训练数据集\n",
    "train_data = MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=64, shuffle=True)\n",
    "# transform：指示加载的数据集应用的数据预处理的规则，shuffle：洗牌，是否打乱输入数据顺序\n",
    "# 测试数据集\n",
    "test_data = MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True)\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "print(\"训练数据集的长度：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度：{}\".format(test_data_size))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a2af8b",
   "metadata": {},
   "source": [
    "在进一步检测一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81431ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像形状: torch.Size([1, 28, 28])\n",
      "标签: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"图像形状:\", train_data[420][0].shape)  # 应该是 torch.Size([1, 28, 28])\n",
    "print(\"标签:\", train_data[420][1])           # 应该是 0-9 的数字"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c86ad2",
   "metadata": {},
   "source": [
    "开始构建模型！\n",
    "\n",
    "模型主要由两个卷积层，两个池化层，以及三个全连接层构成，激活函数使用relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50d8ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MnistModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MnistModel, self).__init__()\n",
    "        # 定义保持不变\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.maxpool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.linear1 = nn.Linear(320, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    # --- 新增：前半段 (特征提取) ---\n",
    "    def features(self, x):\n",
    "        x = self.relu(self.maxpool1(self.conv1(x)))\n",
    "        x = self.relu(self.maxpool2(self.conv2(x)))\n",
    "        # 这里输出的是 [Batch, 20, 4, 4] 的特征图\n",
    "        return x\n",
    "\n",
    "    # --- 新增：后半段 (分类器) ---\n",
    "    def classifier(self, x):\n",
    "        # 接收特征图，先展平\n",
    "        x = x.view(x.size(0), -1) \n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "    # 原来的 forward 只是把两段连起来\n",
    "    def forward(self, x):\n",
    "        feat = self.features(x)\n",
    "        out = self.classifier(feat)\n",
    "        return out\n",
    "\n",
    "# 损失函数CrossentropyLoss\n",
    "model = MnistModel()#实例化\n",
    "criterion = nn.CrossEntropyLoss()   # 交叉熵损失，相当于Softmax+Log+NllLoss\n",
    "# 线性多分类模型Softmax,给出最终预测值对于10个类别出现的概率，Log:将乘法转换为加法，减少计算量，保证函数的单调性\n",
    "# NLLLoss:计算损失，此过程不需要手动one-hot编码，NLLLoss会自动完成\n",
    "\n",
    "# SGD，优化器，梯度下降算法e\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.14)#lr:学习率\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f362035",
   "metadata": {},
   "source": [
    "关于index可以使用两种处理方式，这里选择的是利用enumerate，也可以选择手动递加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "806b04e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型训练\n",
    "def train():\n",
    "    for index, data in enumerate(train_loader):#获取训练数据以及对应标签\n",
    "       input, target = data   # input为输入数据，target为标签\n",
    "       y_predict = model(input) #模型预测\n",
    "       loss = criterion(y_predict, target)\n",
    "       optimizer.zero_grad() #梯度清零。每次都清空，确保只计算当前批次的梯度\n",
    "       loss.backward()#loss值反向传播\n",
    "       optimizer.step()#更新参数\n",
    "       if index % 100 == 0: # 每一百次保存一次模型，输出损失值\n",
    "           torch.save(model.state_dict(), \"./model/model.pkl\")   # 保存模型\n",
    "           torch.save(optimizer.state_dict(), \"./model/optimizer.pkl\")\n",
    "           print(\"训练次数为：{}，损失值为：{}\".format(index, loss.item() ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a168a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 恢复模型和优化器状态\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_11900\\2976271716.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"./model/model.pkl\"))\n",
      "C:\\Users\\MSI\\AppData\\Local\\Temp\\ipykernel_11900\\2976271716.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  optimizer.load_state_dict(torch.load(\"./model/optimizer.pkl\"))  # 这行很重要！\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "# 在训练前创建模型保存目录\n",
    "os.makedirs('./model', exist_ok=True)  # 如果目录不存在就创建\n",
    "\n",
    "# 然后正常训练，第一次不会加载模型\n",
    "if os.path.exists('./model/model.pkl') and os.path.exists('./model/optimizer.pkl'):\n",
    "    model.load_state_dict(torch.load(\"./model/model.pkl\"))\n",
    "    optimizer.load_state_dict(torch.load(\"./model/optimizer.pkl\"))  # 这行很重要！\n",
    "    print(\"✓ 恢复模型和优化器状态\")\n",
    "else:\n",
    "    print(\"✓ 开始新的训练\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf692b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型测试\n",
    "def test():\n",
    "    correct = 0     # 正确预测的个数\n",
    "    total = 0   # 总数\n",
    "    with torch.no_grad():   # 测试不用计算梯度\n",
    "        for data in test_loader:\n",
    "            input, target = data\n",
    "            output = model(input)   # output输出10个预测取值，概率最大的为预测数\n",
    "            probability, predict = torch.max(input=output.data, dim=1)    # 返回一个元祖，第一个为最大概率值，第二个为最大概率值的下标\n",
    "            # loss = criterion(output, target)\n",
    "            total += target.size(0)  # target是形状为（batch_size,1)的矩阵，使用size（0）取出该批的大小\n",
    "            correct += (predict == target).sum().item()  # predict 和target均为（batch_size,1)的矩阵，sum求出相等的个数\n",
    "        print(\"测试准确率为：%.6f\" %(correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0b27adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "————————第1轮测试开始——————\n",
      "训练次数为：0，损失值为：0.00010700365965021774\n",
      "训练次数为：100，损失值为：6.244760879781097e-05\n",
      "训练次数为：200，损失值为：0.002269541844725609\n",
      "训练次数为：300，损失值为：0.004874390549957752\n",
      "训练次数为：400，损失值为：4.483581142267212e-05\n",
      "训练次数为：500，损失值为：0.0008190689259208739\n",
      "训练次数为：600，损失值为：5.862650141352788e-06\n",
      "训练次数为：700，损失值为：0.00012583246279973537\n",
      "训练次数为：800，损失值为：9.534152195556089e-05\n",
      "训练次数为：900，损失值为：0.0002775305765680969\n",
      "测试准确率为：0.991800\n"
     ]
    }
   ],
   "source": [
    "#测试识别函数\n",
    "if __name__ == '__main__':\n",
    "    #训练与测试\n",
    "    for i in range(1):#训练和测试进行15轮\n",
    "        print(\"————————第{}轮测试开始——————\".format (i + 1))\n",
    "        train()\n",
    "        test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
